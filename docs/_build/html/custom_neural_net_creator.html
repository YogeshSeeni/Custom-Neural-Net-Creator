<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>custom_neural_net_creator package &mdash; Custom-Neural-Net-Creator 2.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=60dbed4a"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Custom-Neural-Net-Creator
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">custom_neural_net_creator package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-src.custom_neural_net_creator.activation_functions">custom_neural_net_creator.activation_functions module</a><ul>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_functions.relu"><code class="docutils literal notranslate"><span class="pre">relu()</span></code></a></li>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_functions.relu_derivative"><code class="docutils literal notranslate"><span class="pre">relu_derivative()</span></code></a></li>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_functions.sigmoid"><code class="docutils literal notranslate"><span class="pre">sigmoid()</span></code></a></li>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_functions.sigmoid_derivative"><code class="docutils literal notranslate"><span class="pre">sigmoid_derivative()</span></code></a></li>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_functions.tanh"><code class="docutils literal notranslate"><span class="pre">tanh()</span></code></a></li>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_functions.tanh_prime"><code class="docutils literal notranslate"><span class="pre">tanh_prime()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.custom_neural_net_creator.activation_layer">custom_neural_net_creator.activation_layer module</a><ul>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_layer.ActivationLayer"><code class="docutils literal notranslate"><span class="pre">ActivationLayer</span></code></a><ul>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_layer.ActivationLayer.backward_prop"><code class="docutils literal notranslate"><span class="pre">ActivationLayer.backward_prop()</span></code></a></li>
<li><a class="reference internal" href="#src.custom_neural_net_creator.activation_layer.ActivationLayer.forward_prop"><code class="docutils literal notranslate"><span class="pre">ActivationLayer.forward_prop()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.custom_neural_net_creator.dense">custom_neural_net_creator.dense module</a><ul>
<li><a class="reference internal" href="#src.custom_neural_net_creator.dense.Dense"><code class="docutils literal notranslate"><span class="pre">Dense</span></code></a><ul>
<li><a class="reference internal" href="#src.custom_neural_net_creator.dense.Dense.backward_prop"><code class="docutils literal notranslate"><span class="pre">Dense.backward_prop()</span></code></a></li>
<li><a class="reference internal" href="#src.custom_neural_net_creator.dense.Dense.forward_prop"><code class="docutils literal notranslate"><span class="pre">Dense.forward_prop()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.custom_neural_net_creator.loss_functions">custom_neural_net_creator.loss_functions module</a><ul>
<li><a class="reference internal" href="#src.custom_neural_net_creator.loss_functions.mean_squared_error"><code class="docutils literal notranslate"><span class="pre">mean_squared_error()</span></code></a></li>
<li><a class="reference internal" href="#src.custom_neural_net_creator.loss_functions.mean_squared_error_derivative"><code class="docutils literal notranslate"><span class="pre">mean_squared_error_derivative()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#custom-neural-net-creator-model-module">custom_neural_net_creator.model module</a></li>
<li><a class="reference internal" href="#module-src.custom_neural_net_creator">Module contents</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Custom-Neural-Net-Creator</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">custom_neural_net_creator package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/custom_neural_net_creator.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="custom-neural-net-creator-package">
<h1>custom_neural_net_creator package<a class="headerlink" href="#custom-neural-net-creator-package" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="module-src.custom_neural_net_creator.activation_functions">
<span id="custom-neural-net-creator-activation-functions-module"></span><h2>custom_neural_net_creator.activation_functions module<a class="headerlink" href="#module-src.custom_neural_net_creator.activation_functions" title="Link to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_functions.relu">
<span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.activation_functions.</span></span><span class="sig-name descname"><span class="pre">relu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.activation_functions.relu" title="Link to this definition"></a></dt>
<dd><p>Calculate the ReLU (Rectified Linear Unit) activation function.</p>
<p>This function computes the ReLU activation function for the given input data <cite>x</cite>. The ReLU function
is commonly used in neural networks and returns the maximum of 0 and the input value for each element
of the input array. It effectively introduces non-linearity to the network.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>x (npt.NDArray): The input data.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The output of the ReLU activation function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_functions.relu_derivative">
<span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.activation_functions.</span></span><span class="sig-name descname"><span class="pre">relu_derivative</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.activation_functions.relu_derivative" title="Link to this definition"></a></dt>
<dd><p>Calculate the derivative of the ReLU (Rectified Linear Unit) activation function.</p>
<p>This function computes the derivative of the ReLU activation function for the given input data <cite>x</cite>.
The ReLU derivative is used in backpropagation to calculate gradients during neural network training.
It returns the result of the ReLU derivative applied to each element of the input array.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>x (npt.NDArray): The input data.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The derivative of the ReLU activation function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_functions.sigmoid">
<span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.activation_functions.</span></span><span class="sig-name descname"><span class="pre">sigmoid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.activation_functions.sigmoid" title="Link to this definition"></a></dt>
<dd><p>Calculate the sigmoid activation function.</p>
<p>This function computes the sigmoid activation function for the given input data <cite>x</cite>. The sigmoid
function maps input values to the range (0, 1), making it suitable for binary classification problems.
It returns the result of the sigmoid activation applied to each element of the input array.
Args:</p>
<blockquote>
<div><p>x (npt.NDArray): The input data.</p>
</div></blockquote>
<dl class="simple">
<dt>Returns:</dt><dd><p>npt.NDArray: The output of sigmoid activation function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_functions.sigmoid_derivative">
<span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.activation_functions.</span></span><span class="sig-name descname"><span class="pre">sigmoid_derivative</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.activation_functions.sigmoid_derivative" title="Link to this definition"></a></dt>
<dd><p>Calculate the derivative of the sigmoid activation function.</p>
<p>This function computes the derivative of the sigmoid activation function for the given input data <cite>x</cite>.
The sigmoid derivative is used in backpropagation to calculate gradients during neural network training.
It returns the result of the sigmoid derivative applied to each element of the input array.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>x (npt.NDArray): The input data.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The derivative of the sigmoid activation function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_functions.tanh">
<span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.activation_functions.</span></span><span class="sig-name descname"><span class="pre">tanh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.activation_functions.tanh" title="Link to this definition"></a></dt>
<dd><p>Calculate the hyperbolic tangent (tanh) activation function.</p>
<p>This function computes the hyperbolic tangent (tanh) activation function for the given input data <cite>x</cite>.
The tanh function maps input values to the range (-1, 1) and is used in neural networks to introduce
non-linearity. It returns the result of the tanh activation applied to each element of the input array.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>x (npt.NDArray): The input data.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The output of the tanh activation function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_functions.tanh_prime">
<span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.activation_functions.</span></span><span class="sig-name descname"><span class="pre">tanh_prime</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.activation_functions.tanh_prime" title="Link to this definition"></a></dt>
<dd><p>Calculate the derivative of the hyperbolic tangent (tanh) activation function.</p>
<p>This function computes the derivative of the hyperbolic tangent (tanh) activation function for the given
input data <cite>x</cite>. The tanh derivative is used in backpropagation to calculate gradients during neural
network training. It returns the result of the tanh derivative applied to each element of the input array.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>x (npt.NDArray): The input data.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The derivative of the tanh activation function.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-src.custom_neural_net_creator.activation_layer">
<span id="custom-neural-net-creator-activation-layer-module"></span><h2>custom_neural_net_creator.activation_layer module<a class="headerlink" href="#module-src.custom_neural_net_creator.activation_layer" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_layer.ActivationLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.activation_layer.</span></span><span class="sig-name descname"><span class="pre">ActivationLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation_function</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">relu</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">sigmoid</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tanh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_derivative</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">relu_derivative</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">sigmoid_derivative</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tanh_prime</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.custom_neural_net_creator.activation_layer.ActivationLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_layer.ActivationLayer.backward_prop">
<span class="sig-name descname"><span class="pre">backward_prop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dE_dY</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.activation_layer.ActivationLayer.backward_prop" title="Link to this definition"></a></dt>
<dd><p>This method performs backward propagation through the Activation Layer. It computes the gradient
of the error with respect to the layer’s input by multiplying the activation derivative with
∂E/∂Y. Since there are no parameters to optimize in this layer, it directly returns the gradient
of the error with respect to the input, which can be used for backpropagation in previous layers.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>dE_dY (npt.NDArray): The gradient of the error function.
learning_rate (float): The learning rate for perorming gradient descent.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The gradient of the error with respect to current layer’s input.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.activation_layer.ActivationLayer.forward_prop">
<span class="sig-name descname"><span class="pre">forward_prop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.activation_layer.ActivationLayer.forward_prop" title="Link to this definition"></a></dt>
<dd><p>Perform forward propagation through the Activation Layer.</p>
<p>This method computes the forward propagation of input data through the Activation Layer. It saves
the input <cite>x</cite> for later use in backpropagation and passes the input through the specified activation
function. The result is returned as the output of the layer.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>x (npt.NDArray): The input data for the activation layer.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The output of the activation layer.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.custom_neural_net_creator.dense">
<span id="custom-neural-net-creator-dense-module"></span><h2>custom_neural_net_creator.dense module<a class="headerlink" href="#module-src.custom_neural_net_creator.dense" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.dense.Dense">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.dense.</span></span><span class="sig-name descname"><span class="pre">Dense</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.custom_neural_net_creator.dense.Dense" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.dense.Dense.backward_prop">
<span class="sig-name descname"><span class="pre">backward_prop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dE_dY</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.dense.Dense.backward_prop" title="Link to this definition"></a></dt>
<dd><p>Perform backward propagation through the Dense layer and update weights and biases.</p>
<p>This method performs backward propagation through the Dense layer. It calculates the gradients
of the error with respect to the layer’s weights, biases, and input. The gradients are then used
to update the weights and biases using gradient descent. The method returns the gradient of the
error with respect to the layer’s input, to be used for backpropagation in previous layers.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>dE_dY (npt.NDArray): The gradient of the error function.
learning_rate (float): The learning rate for perorming gradient descent.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The gradient of the error with respect to current layer’s input.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.dense.Dense.forward_prop">
<span class="sig-name descname"><span class="pre">forward_prop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.dense.Dense.forward_prop" title="Link to this definition"></a></dt>
<dd><p>Perform forward propagation through the Dense layer.</p>
<p>This method computes the forward propagation of input data through the Dense layer. 
The output is calculated by performing a dot product between the input and the layer’s weights, 
adding the bias, and returning the result. It saves
the input <cite>x</cite> for later use in backpropagation.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>x (npt.NDArray): The input data of the Dense layer.</p>
</dd>
<dt>Returns:</dt><dd><p>npt.NDArray: The output of the Dense layer.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.custom_neural_net_creator.loss_functions">
<span id="custom-neural-net-creator-loss-functions-module"></span><h2>custom_neural_net_creator.loss_functions module<a class="headerlink" href="#module-src.custom_neural_net_creator.loss_functions" title="Link to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.loss_functions.mean_squared_error">
<span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.loss_functions.</span></span><span class="sig-name descname"><span class="pre">mean_squared_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actual</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.loss_functions.mean_squared_error" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean squared error between acuatl and predicted values.</p>
<p>This function computes the mean squared error (MSE) between the actual and predicted values.
It takes two NumPy arrays, <cite>actual</cite> and <cite>prediction</cite>, and calculates the squared difference 
between corresponding elements, then returns the mean of those squared differences. Lower values 
indicate better performance during training.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>actual (npt.NDArray): The actual values.
prediction (npt.NDArray): The predicted values.</p>
</dd>
<dt>Returns:</dt><dd><p>float: The mean squared error between actual and predicted values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.custom_neural_net_creator.loss_functions.mean_squared_error_derivative">
<span class="sig-prename descclassname"><span class="pre">src.custom_neural_net_creator.loss_functions.</span></span><span class="sig-name descname"><span class="pre">mean_squared_error_derivative</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actual</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">[</span></span><span class="pre">ScalarType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#src.custom_neural_net_creator.loss_functions.mean_squared_error_derivative" title="Link to this definition"></a></dt>
<dd><p>Calculate the derivative of the mean squared error with respect to predicted values.</p>
<p>This function computes the mean squared error (MSE) between the actual and predicted values.
It takes two NumPy arrays, <cite>actual</cite> and <cite>prediction</cite>, and calculates the squared difference 
between corresponding elements, then returns the mean of those squared differences. MSE is a
common metric used to measure the accuracy of regression models. Lower values indicate better 
performance during training.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>actual (npt.NDArray): The actual values.
prediction (npt.NDArray): The predicted values.</p>
</dd>
<dt>Returns:</dt><dd><p>float: The derivative of the mean squared error with respect to predicted values.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="custom-neural-net-creator-model-module">
<h2>custom_neural_net_creator.model module<a class="headerlink" href="#custom-neural-net-creator-model-module" title="Link to this heading"></a></h2>
</section>
<section id="module-src.custom_neural_net_creator">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-src.custom_neural_net_creator" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Yogesh Seenichamy.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>